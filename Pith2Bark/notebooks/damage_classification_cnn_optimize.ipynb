{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756146c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115a9b8",
   "metadata": {},
   "source": [
    "# Damage Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_damage_min_max(damage_fname):\n",
    "    df_damage = pd.read_csv(damage_fname)\n",
    "    \n",
    "    columns = ['min', 'max', 'mean', 'orig', 'diff_min_max', 'diff_orig_mean', 'damage_mean']\n",
    "    df_damage_temp = df_damage.drop(columns=columns)\n",
    "    df_damage_temp.rename(columns={'damage_min_max': 'Damage'}, inplace=True)\n",
    "\n",
    "    df_damage_temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_damage_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05999315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_damage_mean(damage_fname):\n",
    "    df_damage = pd.read_csv(damage_fname)\n",
    "    \n",
    "    columns = ['min', 'max', 'mean', 'orig', 'diff_min_max', 'diff_orig_mean', 'damage_min_max']\n",
    "    df_damage_temp = df_damage.drop(columns=columns)\n",
    "    df_damage_temp.rename(columns={'damage_mean': 'Damage'}, inplace=True)\n",
    "\n",
    "    df_damage_temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_damage_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#damage\n",
    "damage_url = \"damage_dataset/damage.csv\"\n",
    "\n",
    "df_damage_min_max = read_damage_min_max(damage_url)\n",
    "\n",
    "df_damage_mean = read_damage_mean(damage_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_damage_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the file paths for your image data and labels\n",
    "img_size = (128, 128)\n",
    "\n",
    "data_dir = 'damage_dataset/images'\n",
    "\n",
    "# convert the DataFrame to a dictionary with specific column names as key and value\n",
    "label_dict = {k: v for k, v in zip(df['image'], df['Damage'])}\n",
    "\n",
    "# create empty lists to store the image data and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# loop through each image file in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    # load the image file and convert it to a NumPy array\n",
    "    image = load_img(os.path.join(data_dir, filename), target_size=img_size)\n",
    "    image = img_to_array(image)\n",
    "    # preprocess the image by scaling the pixel values to be between 0 and 1\n",
    "    image = image.astype('float32') / 255.0\n",
    "    # add the preprocessed image and its label to the lists\n",
    "    images.append(image)\n",
    "    labels.append(label_dict[filename])\n",
    "\n",
    "# convert the lists to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# print the shape of the data to verify that it's been loaded correctly\n",
    "print('Image data shape:', images.shape) #(148, 64, 64, 3)\n",
    "print('Label data shape:', labels.shape) #(148,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169248a3",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11431c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_percent = 0.9\n",
    "\n",
    "epochs, batch_size = 50, 32\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "METRICS = [\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ead18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_map_normalized(y_true, y_pred):\n",
    "    \n",
    "    class_names = ['Not Damage', 'Damaged']\n",
    "    \n",
    "    skplt.metrics.plot_confusion_matrix(y_true, y_pred,\n",
    "                                        figsize=(4,3),\n",
    "                                        normalize=True)\n",
    "    \n",
    "    # Customize axis tick labels\n",
    "    plt.xticks([0,1], class_names)\n",
    "    plt.yticks([0,1], class_names)\n",
    "    \n",
    "    plt.yticks(rotation=90)\n",
    "    \n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e276a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_map(_cm):\n",
    "    # plot confusion matrix as heatmap\n",
    "    labels = ['Not Damage', 'Damaged']\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    \n",
    "    sns.heatmap(_cm, annot=True, cmap=cmap, xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [32, 64, 128, 256]\n",
    "activations = ['relu', 'tanh', 'sigmoid']\n",
    "kernel_size = [3, 5]\n",
    "learning_rate = [0.001, 0.0001, 0.00001]\n",
    "optimizer = ['Adam', 'RMSprop', 'Nadam']\n",
    "dropout = [0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be804b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37157eb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline():\n",
    "    def __init__(self, _input_dim, units, activation, kernel_size, learning_rate, optimizer, dropout, metrics=METRICS):\n",
    "        # Define the model architecture\n",
    "        model = Sequential()\n",
    "        \n",
    "        # convolutional layer\n",
    "        model.add(Conv2D(units, kernel_size=kernel_size, activation=activation, input_shape=_input_dim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "                  \n",
    "        # flatten output of conv\n",
    "        model.add(Flatten())\n",
    "                  \n",
    "        # hidden layer\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        model.add(Dropout(dropout))\n",
    "                  \n",
    "        # output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        if optimizer == 'Adam':\n",
    "            ops = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        elif optimizer == 'RMSprop':\n",
    "            ops = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "        elif optimizer == 'Nadam':\n",
    "            ops = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "            \n",
    "        # Compile the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=ops, metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size):\n",
    "        history = self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        results = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        return results\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = self.model.predict(X_test)\n",
    "        return predictions\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    def metrics_names(self):\n",
    "        return self.model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc8c41",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc63350",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = images\n",
    "y = labels\n",
    "\n",
    "# split the data into a holdout set and the rest for K-Fold cross-validation\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, train_size=training_percent, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "baseline_values = { 'units': 0, 'activation': '', 'kernel_size': 0, 'learning_rate' : 0.0, 'optimizer': '', 'dropout': 0.0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in units:\n",
    "    for a in activations:\n",
    "        for k in kernel_size:\n",
    "            for l in learning_rate:\n",
    "                for o in optimizer:\n",
    "                    for d in dropout:\n",
    "                        print('\\nUnits:', u, 'Activation:', a, 'Kernel size:', k, 'Learning rate:', l, 'Optimizer:', o, 'Dropout:', d)\n",
    "                        for train_index, val_index in kf.split(X_train):\n",
    "                            # split the dataset into training and validation sets for this fold\n",
    "                            X_train_kf, X_val_kf = X_train[train_index], X_train[val_index]\n",
    "                            y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "                            \n",
    "                            model = Baseline(input_dim, u, a, k, l, o, d)\n",
    "\n",
    "                            history = model.train(X_train_kf, y_train_kf, X_val_kf, y_val_kf, epochs, batch_size)\n",
    "                            if (history.history['val_accuracy'][-1] > best_acc):\n",
    "                                best_acc = history.history['val_accuracy'][-1]\n",
    "                                baseline_values = { 'units': u, 'activation': a, 'kernel_size': k, 'learning_rate' : l, 'optimizer': o, 'dropout': d }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7aabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_acc)\n",
    "print(baseline_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
